# Scoring Reasoning Chains in LIMO

The **LIMO** paper ("Less Is More for Reasoning") introduces a systematic approach for evaluating the **quality of reasoning chains** before selecting them for training. The aim is to ensure that only *high-quality demonstrations* of reasoning are used to fine-tune the model. Below is a detailed explanation of how this scoring works.

---

## 1. Why Score Reasoning Chains?

When generating candidate solutions (reasoning traces) for mathematical problems using large language models, not all solutions are equally good:

* Some are **shallow**, jumping to conclusions too quickly.
* Some are **messy**, with poor logical flow and no checks.
* Others are **well-structured**, detailed, and resemble good human reasoning.

To automatically separate the good from the bad, LIMO introduces a **rule-based scoring system**.

---

## 2. Dimensions of Reasoning Quality

LIMO defines **four key characteristics** that make a reasoning chain high quality:

### a) Elaborated Reasoning (30%)

* **Definition**: Covers the logical steps thoroughly, avoids shortcuts.
* **Good**: Walks through all intermediate steps.
* **Bad**: Skips reasoning, provides a short unexplained answer.
* **Measurement**: Length of the solution text.
* **Weight**: 30% of the total score.

### b) Self-Verification (20%)

* **Definition**: Actively checks intermediate results and logical consistency.
* **Good**: Includes phrases like *“let’s verify”*, *“check again”*.
* **Bad**: Assumes correctness without verification.
* **Measurement**: Frequency of words like *“check”*, *“verify”*.
* **Weight**: 20% of the total score.

### c) Exploratory Approach (25%)

* **Definition**: Considers multiple solution paths before concluding.
* **Good**: Uses phrases like *“perhaps this works”*, *“suppose we try this”*.
* **Bad**: Commits immediately without exploration.
* **Measurement**: Count of tentative words such as *“maybe”*, *“might”*, *“suppose”*.
* **Weight**: 25% of the total score.

### d) Adaptive Granularity (25%)

* **Definition**: Explains difficult steps in detail but keeps easy steps concise.
* **Good**: Provides appropriate levels of detail using logical connectors.
* **Bad**: Either overexplains trivialities or skips hard reasoning.
* **Measurement**: Presence of markers such as *“since”*, *“therefore”*, *“because”*.
* **Weight**: 25% of the total score.

---

## 3. Normalization

Because solutions vary in length, raw counts of keywords would unfairly favor longer solutions. To address this, **all keyword frequencies are normalized by text length**, ensuring a fair comparison across reasoning chains of different sizes.

---

## 4. Scoring Formula

Each reasoning chain receives a weighted score:

$$
\text{Score} = 0.3 \cdot \text{Elaborated} + 0.2 \cdot \text{Verification} + 0.25 \cdot \text{Exploration} + 0.25 \cdot \text{Granularity}
$$

Where each dimension is computed based on normalized text features.

---

## 5. Selection Process

1. For each problem, multiple reasoning chains are generated by different models.
2. Each chain is scored using the rule-based system above.
3. The **highest-scoring chain** for each problem is retained.
4. All (problem, best-solution) pairs are ranked by score.
5. The **top 800 pairs** form the final **LIMO dataset**.

---

## 6. Key Takeaway

The scoring system ensures that the final dataset prioritizes **reasoning quality over quantity**. Instead of merely collecting many examples, LIMO carefully selects solutions that:

* Explain thoroughly,
* Verify steps,
* Explore alternatives, and
* Adjust detail appropriately.

This process makes a small dataset (800 examples) far more effective than much larger but uncurated datasets.
